
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link rel="stylesheet" href="css/main.css">
    <link rel="stylesheet" href="bootstrap/css/bootstrap.min.css"> 
    <title>Nhat-Tan Bui</title>
    <link rel="icon" href="images/favicon.png">
</head>

<body>
    <div class="container">
        <div id="profile">
            <div class="container" id="profile">
                <div class="profile-left">
                    <img src="images/ava.jpg" class="profile-photo">
                </div>
                <div class="profile-right">
                    <h2>Nhat-Tan Bui</h2>
                    <p><p style="font-size: 18px; font-style: italic; color: #A9A9A9;">"True scholar is shown by phronesis, not imagination."</></p>
                    <p style="margin-bottom: 0;" class="social-links">
                        <a href="https://drive.google.com/drive/folders/16_Xy52NQQbpkNUWXNCTkmG-FGDA6lzjS?usp=drive_link">CV</a> |
                        <!-- <a href="mailto:tanb@uark.edu">Mail</a> | -->
                        <a href="https://scholar.google.com/citations?user=FV1sNHAAAAAJ">Google Scholar</a> |
                        <a href="#publication">Publications</a> |
                        <a href="https://www.linkedin.com/in/bntannnn/">LinkedIn</a> |
                        <a href="https://github.com/tanbuinhat">GitHub</a> |
                        <a href="https://x.com/nhattanbui22">X</a> |
                        <a href="https://www.instagram.com/peaceful.of.me/">Instagram</a> |
                        <a href="https://www.facebook.com/nhattan22112000/">Facebook</a> 
                    </p>
                    <p><span class="highlight">Email</span>: tannhatb@andrew.cmu.edu</p>
                </div>
            </div>
        </div>   

        <div class="col-xs-12" id="intro">
            <p>Greetings! My name in Vietnamese is B√πi Nh·∫≠t T√¢n (Ë£¥Êó•Êñ∞), which typically means daily renewal üåï with brightness üîÜ and hope üçÄ. I will join Carnegie Mellon University (CMU) as a visiting scholar in January 2026, working with Prof. <a href="https://www.ri.cmu.edu/ri-faculty/fernando-de-la-torre-frade/">Fernando De la Torre</a>, who is such a mensch and a brilliant researcher.</p>
            <p>Previously, I received my M.S. from the University of Arkansas (UARK) in 2025 and my B.S. from the University of Science, VNU-HCM (HCMUS) and Auckland University of Technology (AUT) in 2022. I graduated from <a href="https://www.historicvietnam.com/former-lycee-chasseloup-laubat/">Le Quy Don High School</a>, the oldest school in Ho Chi Minh City (Saigon).</p>
            <p>Over my academic journey, I have been fortunate to work with wonderful advisors: Prof. <a href="https://engineering.uark.edu/electrical-engineering-computer-science/computer-science-faculty/uid/sgauch/name/Susan+E.+Gauch/">Susan Gauch</a> and Distinguished Prof. <a href="https://jacobsschool.ucsd.edu/people/profile/truong-q-nguyen">Truong Nguyen</a> (UC San Diego) during my time at UARK, and Assoc. Prof. <a href="https://en.hcmus.edu.vn/profile/tran-minh-triet/">Minh-Triet Tran</a> and Dr. <a href="https://www.fit.hcmus.edu.vn/~nnthao/">Ngoc-Thao Nguyen</a> during my time at HCMUS. I'm grateful for the unwavering support of my family, my friends and long-term collaborators: <a href="https://scholar.google.com/citations?hl=vi&user=713F7a8AAAAJ">Hieu Hoang</a>, <a href="https://huyquoctrinh.onrender.com/">Quoc-Huy Trinh</a>, <a href="https://quanmai.github.io/">Quan Mai</a>, <a href="https://vvuonghn.github.io/">Vuong Ho</a>, <a href="https://scholar.google.com/citations?hl=vi&user=L5O1bnUAAAAJ">Hai-Dang Nguyen</a>, and <a href="https://leduy99.github.io/">Duy Le</a>. All of the people mentioned are those to whom I owe a debt of gratitude.</p>
            <p><span class="highlight">Research Keywords:</span> Deep Learning üß†, Computer Vision üëÄ, and Multimodal Learning üéè with applications in Mechanistic Interpretability üîç and Cognitive Linguistic Understanding üìù for Vision-Language Models.</p>

            <center>
                <p>Please feel free to drop me an email if you have any questions about my research, or even me üòÅ</p>
            </center>
            
        </div>


        <div class="col-xs-12"  id="publication">
            <h2>Publications</h2>
            <p class="quote">
                "Research is for curiosity and fun, though practical implications can emerge."</> - Zhi-Hua Zhou<br>
                "Without the love of research, mere knowledge and intelligence cannot make a scientist."</> - Ir√®ne Joliot-Curie</p>
            <p>(* indicates equal contribution)</p>

            <div class="row publication-item">
                <div class="col-xs-12 col-sm-3 col-md-2 publication-image">
                    <img src="images/NeIn.png">
                </div>

                <div class="col-xs-12 col-sm-9 col-md-10 publication-text">
                    <p class="article-title">NeIn: Telling What You Don't Want</p>
                    <p style="margin-bottom: 0;">Conference on Computer Vision and Pattern Recognition Workshops (<span class="highlight">CVPRW 2025</span>) SyntaGen</p> 
                    <p style="margin-bottom: 0;">üèÜ <span style="color: #691616;">Best Paper Award</span></p>
                    <p style="margin-bottom: 0;"><span class="author">Nhat-Tan Bui</span>, Dinh-Hieu Hoang, Quoc-Huy Trinh, Minh-Triet Tran, Truong Nguyen, Susan Gauch</p>
                    <p style="margin-bottom: 0;"><i>TLDR</i>: First Large-Scale Vision-Language Negation Dataset for Text-Guided Image Editing</p>
                    <p><a href="https://arxiv.org/abs/2409.06481"> [Paper]</a><a href="https://tanbuinhat.github.io/NeIn/"> [Project Page]</a></p>
                </div>
            </div>

            <div class="row publication-item">
                <div class="col-xs-12 col-sm-3 col-md-2 publication-image">
                    <img src="images/TSRNet.jpg">
                </div>

                <div class="col-xs-12 col-sm-9 col-md-10 publication-text">
                    <p class="article-title">TSRNet: Simple Framework for Real-time ECG Anomaly Detection with Multimodal Time and Spectrogram Restoration Network</p>
                    <p style="margin-bottom: 0;">International Symposium on Biomedical Imaging (<span class="highlight">ISBI 2024</span>)</p> 
                    <p style="margin-bottom: 0;"><span class="author">Nhat-Tan Bui*</span>, Dinh-Hieu Hoang*, Thinh Phan, Minh-Triet Tran, Brijesh Patel, Donald Adjeroh, Ngan Le</p>
                    <p style="margin-bottom: 0;"><i>TLDR</i>: Time-Spectrogram Fusion with Cross-Attention Mechanism and Prioritizing R-peaks during Inference</p>
                    <p><a href="https://arxiv.org/abs/2312.10187"> [Paper]</a><a href="https://github.com/UARK-AICV/TSRNet"> [Code]</a></p>
                </div>
            </div>

            <div class="row publication-item">
                <div class="col-xs-12 col-sm-3 col-md-2 publication-image">
                    <img src="images/SAM3D.jpg">
                </div>

                <div class="col-xs-12 col-sm-9 col-md-10 publication-text">
                    <p class="article-title">SAM3D: Segment Anything Model in Volumetric Medical Images</p>
                    <p style="margin-bottom: 0;">International Symposium on Biomedical Imaging (<span class="highlight">ISBI 2024</span>)</p> 
                    <p style="margin-bottom: 0;"><span class="author">Nhat-Tan Bui*</span>, Dinh-Hieu Hoang*, Minh-Triet Tran, Gianfranco Doretto, Donald Adjeroh, ..., Ngan Le</p>
                    <p style="margin-bottom: 0;"><i>TLDR</i>: Frozen SAM‚Äôs Encoder, Stack Output Slice Embeddings, and Decode with a 3D CNN Decoder</p>
                    <p><a href="https://arxiv.org/abs/2309.03493"> [Paper]</a><a href="https://github.com/UARK-AICV/SAM3D"> [Code]</a></p>
                </div>
            </div>


            <div class="row publication-item">
                <div class="col-xs-12 col-sm-3 col-md-2 publication-image">
                    <img src="images/MEGANet.jpg">
                </div>

                <div class="col-xs-12 col-sm-9 col-md-10 publication-text">
                    <p class="article-title">MEGANet: Multi-Scale Edge-Guided Attention Network for Weak Boundary Polyp Segmentation</p>
                    <p style="margin-bottom: 0;">Winter Conference on Applications of Computer Vision (<span class="highlight">WACV 2024</span>)</p> 
                    <p style="margin-bottom: 0;"><span class="author">Nhat-Tan Bui</span>, Dinh-Hieu Hoang, Quang-Thuc Nguyen, Minh-Triet Tran, Ngan Le</p>
                    <p style="margin-bottom: 0;"><i>TLDR</i>: Laplacian-based Edge Information Fused with Attention Mechanism for UNet Skip Connection</p>
                    <p><a href="https://arxiv.org/abs/2309.03329"> [Paper]</a><a href="https://github.com/UARK-AICV/MEGANet"> [Code]</a></p>
                </div>
            </div>
            
        </div>


        <div class="col-xs-12" id="documents">
            <h2>Documents</h2>
            <p class="quote">"What I cannot create, I do not understand."</> - Richard Feynman</p>
            <ul>
                <li><a href="https://drive.google.com/file/d/1BZ5nhnDz6Wyke-MJ8NSvJ7gB6AqwKMNn/view?usp=sharing">Introduction to Neural Networks for Natural Language Processing</a></li>
                <li><a href="https://drive.google.com/file/d/1oxoGDTKHzduneCsjd1_vYH80rLHePh2l/view?usp=sharing">Recurrent Neural Network and its Variants</a></li>
                <li><a href="https://drive.google.com/file/d/1zu3jAroVLUYrn-IvVmHPqUn3wFqWlLka/view?usp=sharing">Sequence-to-Sequence and Attention</a></li>
                <li><a href="https://drive.google.com/file/d/1wyZsNoutCUk3SFocXcAwZsvvgxLwMb8A/view?usp=sharing">Transformer</a></li>
                <li><a href="https://drive.google.com/file/d/1ORIOd24dm9jxn-ORWn47mVZruFvTWpwh/view?usp=sharing">Contextualized Representations and Pretraining Language Models</a></li>
            </ul>
        </div>

        <div class="col-xs-12" id="misc">
            <h2 style="margin-top: 30px;">Credits/Misc</h2>
            <ul style="color: #4F6371;">
                <li>Photo credits to one of my best friends, <a href="https://www.instagram.com/momonguyenn/">Momo Nguyen</a>. We have been friends for almost 10 years!</li>
                <li>Template idea credits to <a href="https://www.xunhuang.me/">Xun Huang</a>, whose work has had a great influence on my research journey. I read his paper on <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.pdf">Adaptive Instance Normalization</a> back in 2022, and from that moment, I knew I wanted to become someone like him.</li>
                <li>My mom is half Chinese, so I am one-quarter Chinese.</li>
                <li>I am a huge fan of Shan Yichun, especially her song ‚ÄúContinued‚Äù (Áª≠ÂÜô).</li>
            </ul>
        </div>

    </div>
</body>
</html>

